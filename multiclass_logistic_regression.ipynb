{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "multiclass_logistic_regression.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ei2bw2nmuyMp",
        "colab_type": "text"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1WF7iyWtXIVp5_HMlCM7VzXseFkEppm94?usp=sharing)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YGb7BPXkCnmP",
        "colab_type": "text"
      },
      "source": [
        "# Introducing Multi - class Logistic Regression\n",
        "We have already learned about logistic regression in [Week#3_Day#6](https://www.google.com/url?q=https://docs.google.com/presentation/d/10mkWll_9s8LkZpy-SzRWQjFr2ay6VppgpjkBVs2HSvM/edit?usp%3Dsharing&sa=D&ust=1592317719588000&usg=AFQjCNFKHARql8iSSd6XH09heRI58Gkk7Q) learning module and build logistic regression model on insurance data in [this notebook](https://www.google.com/url?q=https://github.com/dphi-official/ML_Models/blob/master/Logistic_Regression/logistic_regression.ipynb&sa=D&ust=1592317719569000&usg=AFQjCNGLPKpHwzSZWpRTauOb8vUv7cEmVA). In the earlier notebook we had build binary logistic regression as the target variable (i.e. bought_insurance) in insurance data has two classes 1 (bought insurance) and 0 (didn't buy insurance). In this notebook we will talk about multi - class logistic regression.\n",
        "\n",
        "**Multi - class Logistic Regression:** Here the target variable has more than two possible classes/categories. For example, salary of an employee can be categorized as **'low', 'medium' and 'high'**. There are two types of multi - class logistic regression:\n",
        "\n",
        "1. **Multinomial Logistic Regression:** \n",
        "The target variable has three or more classes/categories which are not in any particular order. So, there are three or more nominal categories. \n",
        "Examples: Fruits (apple, mango, orange and banana), profession (e.g., with five groups: surgeon, doctor, nurse, dentist, therapist)\n",
        "\n",
        "2. **Ordinal Logistic Regression:**\n",
        "The target variable has three or more ordinal categories. So, there is intrinsic order involved with the categories. \n",
        "For example, the student performance can be categorized as poor, average, good and excellent, the salary of an employee can be categorized as **'low', 'medium' and 'high'**\n",
        "\n",
        "## Agenda\n",
        "*  About Dataset\n",
        "*  Loading Libraries and Data\n",
        "*  Understanding the Data\n",
        "*  Separating Input and Output Variables\n",
        "*  Splitting Data into Train and Test Sets\n",
        "*  Build Model\n",
        "*  Prediction\n",
        "*  Check Model Performace\n",
        "\n",
        "\n",
        "## About Dataset\n",
        "I hope all of you guys remembered the wine dataset on which we have done exploratory data analysis. Here we will take only red wine data. Given different physiochemical tests, we want to predict the quality of wine in range 1 to 10.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQYAUiwVNfe6",
        "colab_type": "text"
      },
      "source": [
        "## Loading Libraries\n",
        "All Python capabilities are not loaded to our working environment by default (even they are already installed in your system). So, we import each and every library that we want to use.\n",
        "\n",
        "In data science, numpy and pandas are most commonly used libraries. Numpy is required for calculations like means, medians, square roots, etc. Pandas is used for data processin and data frames. We chose alias names for our libraries for the sake of our convenience (numpy --> np and pandas --> pd)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LnyoDUYlCfgq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np        # Fundamental package for linear algebra and multidimensional arrays\n",
        "import pandas as pd       # Data analysis and manipultion tool\n",
        "\n",
        "# To ignore warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IDu_ZilmOf1H",
        "colab_type": "text"
      },
      "source": [
        "## Loading Data\n",
        "Pandas module is used for reading files. We have our data in '.csv' format. We will use 'read_csv()' function for loading the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gl5O8JvJOVXG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# In read_csv() function, we have passed the location to where the files are located in the UCI website. The data is separated by ';'\n",
        "# so we used separator as ';' (sep = \";\")\n",
        "red_wine_data = pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\", sep=\";\")"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T6wTwfWgO5Le",
        "colab_type": "text"
      },
      "source": [
        "## Understanding Data\n",
        "Let's see how our data looks."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YYc_H0DTOr5e",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "50cb13e3-5b03-4cf9-a9a1-f8b5cee1bda0"
      },
      "source": [
        "# Red Wine\n",
        "red_wine_data.head() "
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>fixed acidity</th>\n",
              "      <th>volatile acidity</th>\n",
              "      <th>citric acid</th>\n",
              "      <th>residual sugar</th>\n",
              "      <th>chlorides</th>\n",
              "      <th>free sulfur dioxide</th>\n",
              "      <th>total sulfur dioxide</th>\n",
              "      <th>density</th>\n",
              "      <th>pH</th>\n",
              "      <th>sulphates</th>\n",
              "      <th>alcohol</th>\n",
              "      <th>quality</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>7.4</td>\n",
              "      <td>0.70</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.9</td>\n",
              "      <td>0.076</td>\n",
              "      <td>11.0</td>\n",
              "      <td>34.0</td>\n",
              "      <td>0.9978</td>\n",
              "      <td>3.51</td>\n",
              "      <td>0.56</td>\n",
              "      <td>9.4</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>7.8</td>\n",
              "      <td>0.88</td>\n",
              "      <td>0.00</td>\n",
              "      <td>2.6</td>\n",
              "      <td>0.098</td>\n",
              "      <td>25.0</td>\n",
              "      <td>67.0</td>\n",
              "      <td>0.9968</td>\n",
              "      <td>3.20</td>\n",
              "      <td>0.68</td>\n",
              "      <td>9.8</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>7.8</td>\n",
              "      <td>0.76</td>\n",
              "      <td>0.04</td>\n",
              "      <td>2.3</td>\n",
              "      <td>0.092</td>\n",
              "      <td>15.0</td>\n",
              "      <td>54.0</td>\n",
              "      <td>0.9970</td>\n",
              "      <td>3.26</td>\n",
              "      <td>0.65</td>\n",
              "      <td>9.8</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>11.2</td>\n",
              "      <td>0.28</td>\n",
              "      <td>0.56</td>\n",
              "      <td>1.9</td>\n",
              "      <td>0.075</td>\n",
              "      <td>17.0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>0.9980</td>\n",
              "      <td>3.16</td>\n",
              "      <td>0.58</td>\n",
              "      <td>9.8</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>7.4</td>\n",
              "      <td>0.70</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.9</td>\n",
              "      <td>0.076</td>\n",
              "      <td>11.0</td>\n",
              "      <td>34.0</td>\n",
              "      <td>0.9978</td>\n",
              "      <td>3.51</td>\n",
              "      <td>0.56</td>\n",
              "      <td>9.4</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   fixed acidity  volatile acidity  citric acid  ...  sulphates  alcohol  quality\n",
              "0            7.4              0.70         0.00  ...       0.56      9.4        5\n",
              "1            7.8              0.88         0.00  ...       0.68      9.8        5\n",
              "2            7.8              0.76         0.04  ...       0.65      9.8        5\n",
              "3           11.2              0.28         0.56  ...       0.58      9.8        6\n",
              "4            7.4              0.70         0.00  ...       0.56      9.4        5\n",
              "\n",
              "[5 rows x 12 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7GZNwmuZVVuX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "3aa40651-3461-4843-ebad-3d33e4285ed7"
      },
      "source": [
        "red_wine_data.columns"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar',\n",
              "       'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density',\n",
              "       'pH', 'sulphates', 'alcohol', 'quality'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yUzJB6ALUZvR",
        "colab_type": "text"
      },
      "source": [
        "### Different attributes\n",
        "**Input variables (based on physicochemical tests):**\n",
        "1. fixed acidity\n",
        "2. volatile acidity\n",
        "3. citric acid\n",
        "4. residual sugar\n",
        "5. chlorides\n",
        "6. free sulfur dioxide\n",
        "7. total sulfur dioxide\n",
        "8. density\n",
        "9.  pH\n",
        "10.  sulphates\n",
        "11.  alcohol\n",
        "**Output variable (based on sensory data):**\n",
        "12. quality (score between 0 and 10)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9jgMymMeRCBe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 314
        },
        "outputId": "4117e621-b80d-4660-ce75-ef6fcd8f538d"
      },
      "source": [
        "# Basic statistical details about data\n",
        "red_wine_data.describe()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>fixed acidity</th>\n",
              "      <th>volatile acidity</th>\n",
              "      <th>citric acid</th>\n",
              "      <th>residual sugar</th>\n",
              "      <th>chlorides</th>\n",
              "      <th>free sulfur dioxide</th>\n",
              "      <th>total sulfur dioxide</th>\n",
              "      <th>density</th>\n",
              "      <th>pH</th>\n",
              "      <th>sulphates</th>\n",
              "      <th>alcohol</th>\n",
              "      <th>quality</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>1599.000000</td>\n",
              "      <td>1599.000000</td>\n",
              "      <td>1599.000000</td>\n",
              "      <td>1599.000000</td>\n",
              "      <td>1599.000000</td>\n",
              "      <td>1599.000000</td>\n",
              "      <td>1599.000000</td>\n",
              "      <td>1599.000000</td>\n",
              "      <td>1599.000000</td>\n",
              "      <td>1599.000000</td>\n",
              "      <td>1599.000000</td>\n",
              "      <td>1599.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>8.319637</td>\n",
              "      <td>0.527821</td>\n",
              "      <td>0.270976</td>\n",
              "      <td>2.538806</td>\n",
              "      <td>0.087467</td>\n",
              "      <td>15.874922</td>\n",
              "      <td>46.467792</td>\n",
              "      <td>0.996747</td>\n",
              "      <td>3.311113</td>\n",
              "      <td>0.658149</td>\n",
              "      <td>10.422983</td>\n",
              "      <td>5.636023</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>1.741096</td>\n",
              "      <td>0.179060</td>\n",
              "      <td>0.194801</td>\n",
              "      <td>1.409928</td>\n",
              "      <td>0.047065</td>\n",
              "      <td>10.460157</td>\n",
              "      <td>32.895324</td>\n",
              "      <td>0.001887</td>\n",
              "      <td>0.154386</td>\n",
              "      <td>0.169507</td>\n",
              "      <td>1.065668</td>\n",
              "      <td>0.807569</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>4.600000</td>\n",
              "      <td>0.120000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.900000</td>\n",
              "      <td>0.012000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>0.990070</td>\n",
              "      <td>2.740000</td>\n",
              "      <td>0.330000</td>\n",
              "      <td>8.400000</td>\n",
              "      <td>3.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>7.100000</td>\n",
              "      <td>0.390000</td>\n",
              "      <td>0.090000</td>\n",
              "      <td>1.900000</td>\n",
              "      <td>0.070000</td>\n",
              "      <td>7.000000</td>\n",
              "      <td>22.000000</td>\n",
              "      <td>0.995600</td>\n",
              "      <td>3.210000</td>\n",
              "      <td>0.550000</td>\n",
              "      <td>9.500000</td>\n",
              "      <td>5.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>7.900000</td>\n",
              "      <td>0.520000</td>\n",
              "      <td>0.260000</td>\n",
              "      <td>2.200000</td>\n",
              "      <td>0.079000</td>\n",
              "      <td>14.000000</td>\n",
              "      <td>38.000000</td>\n",
              "      <td>0.996750</td>\n",
              "      <td>3.310000</td>\n",
              "      <td>0.620000</td>\n",
              "      <td>10.200000</td>\n",
              "      <td>6.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>9.200000</td>\n",
              "      <td>0.640000</td>\n",
              "      <td>0.420000</td>\n",
              "      <td>2.600000</td>\n",
              "      <td>0.090000</td>\n",
              "      <td>21.000000</td>\n",
              "      <td>62.000000</td>\n",
              "      <td>0.997835</td>\n",
              "      <td>3.400000</td>\n",
              "      <td>0.730000</td>\n",
              "      <td>11.100000</td>\n",
              "      <td>6.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>15.900000</td>\n",
              "      <td>1.580000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>15.500000</td>\n",
              "      <td>0.611000</td>\n",
              "      <td>72.000000</td>\n",
              "      <td>289.000000</td>\n",
              "      <td>1.003690</td>\n",
              "      <td>4.010000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>14.900000</td>\n",
              "      <td>8.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       fixed acidity  volatile acidity  ...      alcohol      quality\n",
              "count    1599.000000       1599.000000  ...  1599.000000  1599.000000\n",
              "mean        8.319637          0.527821  ...    10.422983     5.636023\n",
              "std         1.741096          0.179060  ...     1.065668     0.807569\n",
              "min         4.600000          0.120000  ...     8.400000     3.000000\n",
              "25%         7.100000          0.390000  ...     9.500000     5.000000\n",
              "50%         7.900000          0.520000  ...    10.200000     6.000000\n",
              "75%         9.200000          0.640000  ...    11.100000     6.000000\n",
              "max        15.900000          1.580000  ...    14.900000     8.000000\n",
              "\n",
              "[8 rows x 12 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FX7y5M8pWW9m",
        "colab_type": "text"
      },
      "source": [
        "Let's see target variable 'quality'."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VdbepaGiWbDu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "f8387097-6a33-41d6-d136-bdbe854d33fa"
      },
      "source": [
        "red_wine_data.quality.value_counts().plot(kind = 'bar')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7efdcfecd208>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FzyzsV5RWjR8",
        "colab_type": "text"
      },
      "source": [
        "We can observe here more wines are of average quality than poor quality and good quality. This is what we had observed in our EDA notebook of wine data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F7UacnWPVsYP",
        "colab_type": "text"
      },
      "source": [
        "We have already done the EDA part of this dataset in our earlier notebook. So we will not dive into EDA more here. Let's separate the independent and dependent variables."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tqZidFhVV8EV",
        "colab_type": "text"
      },
      "source": [
        "### Separating Input Features and Output Features\n",
        "Before building any machine learning model, we always separate the input variables and output variables. Input variables are those quantities whose values are changed naturally in an experiment, whereas output variable is the one whose values are dependent on the input variables. So, input variables are also known as independent variables as its values are not dependent on any other quantity, and output variable/s are also known as dependent variables as its values are dependent on other variable i.e. input variables. Like here in this data, we can see that whether a person will buy insurance or not is dependent on the age of that person\n",
        "\n",
        "By convention input variables are represented with 'X' and output variables are represented with 'y'."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F8vuUC95Rpm3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Input/independent variables\n",
        "X = red_wine_data.drop('quality', axis = 1)   # her we are droping the quality feature as this is the target and 'X' is input features, the changes are not \n",
        "                                              # made inplace as we have not used 'inplace = True'\n",
        "\n",
        "y = red_wine_data.quality             # Output/Dependent variable"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dz-95nrxXKFX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "0bc2ac27-c104-4ef8-8838-fba0b05ab6dc"
      },
      "source": [
        "# Let's check the shapes of X and y\n",
        "print(\"Shape: \", X.shape, \"Dimension: \", X.ndim)\n",
        "print(\"Shape: \", y.shape, \"Dimension: \", y.ndim)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape:  (1599, 11) Dimension:  2\n",
            "Shape:  (1599,) Dimension:  1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kV17WuFaXjf1",
        "colab_type": "text"
      },
      "source": [
        "We had discussed in the earlier notebook that input variable must be a 2D array and target of 1D array."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ke41Lb-zYAcb",
        "colab_type": "text"
      },
      "source": [
        "### Splitting the data into Train and Test Set\n",
        "We want to check the performance of the model that we built. For this purpose, we always split (both input and output data) the given data into training set which will be used to train the model, and test set which will be used to check how accurately the model is predicting outcomes.\n",
        "\n",
        "For this purpose we have a class called 'train_test_split' in the 'sklearn.model_selection' module."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f55Wc1s1XidB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import train_test_split\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V3S-WWcwYI86",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3, random_state = 42)\n",
        "\n",
        "# X_train: independent/input feature data for training the model\n",
        "# y_train: dependent/output feature data for training the model\n",
        "# X_test: independent/input feature data for testing the model; will be used to predict the output values\n",
        "# y_test: original dependent/output values of X_test; We will compare this values with our predicted values to check the performance of our built model.\n",
        " \n",
        "# test_size = 0.30: 30% of the data will go for test set and 70% of the data will go for train set\n",
        "# random_state = 42: this will fix the split i.e. there will be same split for each time you run the code"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1MCJEiWPYPrU",
        "colab_type": "text"
      },
      "source": [
        "## Building Model\n",
        "Now we are finally ready, and we can train the model.\n",
        "\n",
        "First, we need to import our model - Logistic Regression (again, using the sklearn library).\n",
        "\n",
        "Then we would feed the model both with the data (X_train) and the answers for that data (y_train)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PW-t4hTeYMnE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import Logistic Regression from sklearn.linear_model\n",
        "from sklearn.linear_model import LogisticRegression"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CJAwU6RdYT3c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "log_model = LogisticRegression()"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7b7PRDhmY8J1",
        "colab_type": "text"
      },
      "source": [
        "### !!!Warning\n",
        "Different columns in this dataset are in different scales. One may get 'ConvergenceWarning' here while fitting the model. Please ignore this for here. If you want to get rid of this error then scale your data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9xmQtKTbYYbB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "dfa04b6a-59f1-4519-9896-68915876069e"
      },
      "source": [
        "# Fit the model\n",
        "log_model.fit(X_train, y_train)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
              "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
              "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
              "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
              "                   warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W49ndZ6-ZniC",
        "colab_type": "text"
      },
      "source": [
        "The training happens in the third line (the \"fit\" function)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-ShX9-uaAWy",
        "colab_type": "text"
      },
      "source": [
        "**Point to be noted:**\n",
        "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
        "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
        "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
        "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
        "                   warm_start=False)\n",
        "\n",
        "If you clearly observe the above output that you got after fitting the model, there is an argument called 'multi_class' which is 'auto' by default. If you go back and see the insurance data, the argument 'multi_class' was still 'auto'. Here 'auto' does automatic selection of binary and multinomial. If the data is binary classification 'auto' does binary classification, and if the data is multi classification, 'auto' does multi classification. \n",
        "\n",
        "Note: You can also change 'auto' to 'ovr'. Here 'ovr' does only binary classificaion.\n",
        "\n",
        "Further details: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ocweqF_McEWN",
        "colab_type": "text"
      },
      "source": [
        "### Prediction\n",
        "Now logistic regression model (i.e. log_model) is trained using X_train and y_trian data. Let's predict the target value (i.e. the quality of wine) for the X_test data. We use \"predict()\" method for prediction."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EVi4HHKaYbcN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions = log_model.predict(X_test)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DorCZd7DcMqK",
        "colab_type": "text"
      },
      "source": [
        "We already have actual target values (i.e. y_test) for X_test. Let's compare y_test and the predicted value for X_test by our log_model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vFZIhFU8cIe-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "f3d57eb4-fb49-4996-efd1-4045456040a2"
      },
      "source": [
        "y_test.values"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([6, 5, 6, 5, 6, 5, 5, 5, 5, 6, 7, 3, 5, 5, 6, 7, 5, 7, 8, 5, 5, 6,\n",
              "       5, 6, 6, 6, 7, 6, 5, 6, 5, 5, 6, 5, 6, 5, 7, 5, 4, 6, 5, 5, 7, 5,\n",
              "       5, 6, 7, 6, 5, 6, 5, 5, 5, 7, 6, 6, 6, 5, 5, 5, 5, 7, 5, 6, 6, 5,\n",
              "       6, 5, 6, 5, 6, 4, 6, 6, 6, 5, 8, 5, 6, 6, 5, 6, 5, 6, 6, 7, 5, 6,\n",
              "       7, 4, 7, 6, 5, 5, 5, 6, 5, 6, 5, 6, 5, 5, 5, 7, 6, 7, 6, 5, 6, 5,\n",
              "       8, 5, 6, 5, 6, 7, 6, 6, 5, 6, 6, 6, 6, 6, 6, 6, 7, 6, 5, 5, 6, 5,\n",
              "       5, 5, 6, 5, 5, 5, 5, 6, 7, 6, 8, 5, 5, 5, 6, 6, 6, 5, 6, 7, 6, 5,\n",
              "       6, 5, 5, 6, 6, 6, 7, 5, 7, 5, 5, 5, 6, 6, 5, 5, 6, 5, 7, 6, 7, 6,\n",
              "       6, 5, 5, 6, 4, 6, 5, 7, 5, 5, 4, 5, 7, 6, 5, 6, 6, 7, 6, 5, 5, 6,\n",
              "       5, 7, 5, 6, 6, 5, 7, 5, 5, 5, 6, 7, 7, 5, 5, 6, 6, 7, 6, 5, 6, 6,\n",
              "       6, 6, 6, 7, 4, 5, 5, 7, 5, 5, 5, 5, 6, 6, 5, 7, 5, 6, 6, 6, 5, 4,\n",
              "       6, 7, 6, 7, 5, 6, 6, 5, 5, 6, 5, 6, 4, 5, 6, 6, 5, 6, 6, 5, 5, 6,\n",
              "       7, 7, 6, 5, 6, 6, 5, 6, 5, 6, 5, 5, 5, 6, 6, 6, 7, 5, 5, 6, 5, 7,\n",
              "       5, 6, 4, 6, 6, 8, 6, 5, 5, 6, 5, 7, 6, 6, 5, 5, 7, 6, 6, 5, 6, 6,\n",
              "       5, 7, 6, 6, 6, 6, 5, 6, 5, 5, 6, 4, 6, 6, 6, 5, 5, 5, 6, 6, 6, 6,\n",
              "       4, 7, 6, 6, 6, 5, 6, 7, 5, 5, 6, 7, 5, 5, 6, 5, 6, 5, 6, 5, 5, 6,\n",
              "       5, 6, 6, 6, 5, 6, 4, 5, 4, 5, 5, 6, 5, 6, 6, 5, 5, 5, 5, 5, 6, 5,\n",
              "       6, 6, 6, 5, 5, 6, 5, 5, 6, 6, 6, 7, 6, 5, 5, 6, 6, 5, 5, 6, 7, 6,\n",
              "       5, 6, 5, 7, 5, 5, 7, 5, 6, 7, 7, 6, 6, 5, 6, 6, 7, 6, 5, 7, 6, 6,\n",
              "       6, 5, 5, 5, 5, 5, 6, 5, 5, 5, 7, 6, 7, 6, 4, 5, 7, 5, 5, 5, 6, 6,\n",
              "       6, 6, 6, 5, 6, 5, 6, 5, 6, 6, 7, 4, 6, 5, 6, 6, 7, 5, 7, 5, 5, 6,\n",
              "       5, 5, 6, 5, 6, 5, 5, 6, 6, 4, 5, 6, 5, 7, 8, 6, 7, 4])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mlAZxEyTcPcX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "04e0baef-7006-47af-900d-a5c5abc9a180"
      },
      "source": [
        "predictions"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([5, 5, 5, 5, 6, 5, 5, 5, 6, 6, 6, 5, 5, 5, 5, 6, 5, 5, 6, 5, 6, 5,\n",
              "       6, 6, 5, 5, 6, 5, 5, 6, 5, 6, 6, 5, 5, 5, 6, 6, 6, 6, 5, 5, 6, 5,\n",
              "       6, 6, 6, 5, 5, 6, 5, 5, 6, 6, 5, 5, 6, 5, 6, 5, 5, 6, 5, 5, 6, 5,\n",
              "       6, 5, 6, 5, 6, 5, 6, 6, 6, 5, 6, 6, 6, 6, 5, 6, 5, 6, 6, 6, 5, 6,\n",
              "       6, 5, 6, 5, 6, 6, 5, 6, 5, 6, 5, 5, 5, 5, 6, 6, 6, 6, 5, 5, 6, 5,\n",
              "       6, 5, 6, 5, 6, 6, 6, 5, 5, 6, 6, 5, 5, 5, 5, 5, 6, 6, 5, 6, 6, 5,\n",
              "       5, 6, 6, 5, 5, 5, 5, 6, 5, 6, 6, 6, 6, 5, 6, 6, 6, 5, 6, 6, 5, 6,\n",
              "       6, 6, 5, 6, 5, 6, 6, 6, 6, 5, 5, 6, 5, 5, 5, 5, 5, 5, 6, 5, 5, 6,\n",
              "       6, 5, 5, 5, 5, 6, 5, 7, 5, 6, 6, 6, 6, 5, 6, 6, 6, 6, 6, 5, 5, 5,\n",
              "       5, 6, 5, 5, 5, 5, 6, 6, 5, 5, 5, 6, 6, 5, 6, 6, 6, 6, 5, 5, 6, 5,\n",
              "       5, 6, 6, 6, 5, 5, 5, 6, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 5, 5, 6, 5,\n",
              "       6, 6, 6, 5, 6, 5, 7, 5, 6, 6, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 5, 7,\n",
              "       6, 6, 5, 5, 6, 6, 5, 6, 5, 5, 5, 6, 6, 6, 6, 5, 6, 5, 5, 5, 5, 6,\n",
              "       5, 6, 5, 6, 5, 7, 5, 5, 5, 6, 5, 6, 6, 6, 6, 5, 6, 5, 5, 5, 6, 6,\n",
              "       6, 6, 6, 6, 5, 5, 5, 6, 5, 5, 6, 5, 6, 6, 5, 5, 5, 5, 6, 6, 5, 6,\n",
              "       6, 6, 5, 5, 5, 6, 6, 6, 5, 5, 6, 6, 6, 5, 6, 5, 6, 5, 6, 6, 5, 6,\n",
              "       5, 5, 5, 5, 5, 5, 5, 6, 6, 5, 6, 6, 6, 6, 6, 5, 5, 5, 5, 5, 6, 6,\n",
              "       5, 6, 5, 6, 5, 5, 5, 6, 6, 5, 6, 6, 6, 5, 5, 6, 6, 6, 5, 5, 6, 6,\n",
              "       6, 5, 5, 6, 5, 5, 6, 5, 5, 6, 6, 6, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6,\n",
              "       6, 6, 6, 5, 5, 5, 5, 6, 5, 5, 6, 5, 6, 5, 5, 5, 6, 5, 5, 6, 5, 6,\n",
              "       6, 6, 6, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 5, 5, 6, 6, 5, 6, 5, 5, 5,\n",
              "       6, 5, 6, 5, 5, 5, 6, 6, 6, 6, 6, 6, 5, 6, 6, 6, 6, 5])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N03lPHnjcYFC",
        "colab_type": "text"
      },
      "source": [
        "### Model Performance\n",
        "We can also check how accurate our model is performing using the 'accuracy_score' class from 'sklearn.metrics'."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zgHXp0zY3wB3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "ce996488-38e0-42df-ef78-f3e5de33dd7b"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "confusion_matrix(y_test, predictions)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  0,   0,   1,   0,   0,   0],\n",
              "       [  0,   0,  11,   6,   0,   0],\n",
              "       [  0,   0, 139,  56,   0,   0],\n",
              "       [  0,   0,  77, 121,   2,   0],\n",
              "       [  0,   0,   4,  56,   1,   0],\n",
              "       [  0,   0,   0,   5,   1,   0]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WllwJsct4Oln",
        "colab_type": "text"
      },
      "source": [
        "Again if you observe here, the class wise false positives (above the main diagonal) and the class wise false negatives (below the main diagonal) are almost symmetrical. So, the accuracy score is an important metric here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yJFT1qOicTWj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import accuracy_score"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "992-t06DccS6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6e37f8f0-526d-4854-f055-9009a80a48f4"
      },
      "source": [
        "accuracy_score(y_test, predictions)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.54375"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DETRhfShcgyb",
        "colab_type": "text"
      },
      "source": [
        "Our model is predicting 54.37% correct results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fn1yJeMXctqy",
        "colab_type": "text"
      },
      "source": [
        "**Thanks for reading the Notebook!!!**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x9X6ggkAc1ET",
        "colab_type": "text"
      },
      "source": [
        "## Exercise\n",
        "Use raw data link of iris data: https://raw.githubusercontent.com/dphi-official/Datasets/master/iris.csv\n",
        "\n",
        "**Exercises**\n",
        "* Train Logistic Regression Model for this dataset\n",
        "* Predict the output for test data\n",
        "* Find out the accuracy of the model you built."
      ]
    }
  ]
}